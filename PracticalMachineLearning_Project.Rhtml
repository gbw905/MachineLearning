<html>

<head>
<title>Practical Machine Learning: Prediction Assignment Writeup</title>
</head>

<body>
<h1>Practical Machine Learning: Prediction Assignment Writeup</h1>
<h2>1.0 Introduction</h2>
<h3>1.1 Purpose</h3>
<p>To use data from accelerometers worn on the belt, forearm, arm, or dumbell of six participants to identify how well each participant performed their activity.</p>
<h3>1.2 Background</h3>
<p>Using devices such as Jawbone UP, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity. Common smartphone apps enable individuals to use this data to track how much of a particular activity they perform. So far, smartphone apps are not able to track how well individuals perform their activities.</p>
<p>In this analysis we seek to use R to categorize each individual's experience with each activity to a letter grade, analogous to grades in primary school.</p>
<h3>1.3 R Environment</h3>
<p>In order to complete this project, we must load the following libraries.</p>
<!--begin.rcode
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
end.rcode-->
<p>In order to be able to reproduce the results on demand, we must set the 'seed' used by the random number generator (Yes, I know that means the random numbers are not really random).</p>
<!--begin.rcode
set.seed(1234)
end.rcode-->
<h2>2.0 Data</h2>
<h3>2.1 Data Sources</h3>
<p>The 'Human Activity Recognition' (HAR) group has kindly provided the data for this project. You can read more details about this group on their <a href="http://groupware.les.inf.puc-rio.br/har">website</a>. We will load the two files, 'training' and 'testing'. Note that the 'test' file provided is intended for a test by an external party (what software developers typically call an 'acceptanct' test) and that we still need to partion the 'training' data into 'train' and'test' partitions so that we can use the 'test' partition for our own test -- what developers commonly call a 'unit test'.</p>
<!--begin.rcode
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
test <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
end.rcode-->
<h3>2.2 Partition the Training Dataset for Cross-Validation</h3>
<p>Here, I partition to the 'pml-training.csv' file to create the file 'train' and 'test' files that I will use as part of developing the analysis. This technique is called 'cross-validation'.</p>
<p>Cross-validation is a statistical technique to estimate the prediction error rate by splitting the data into training and test datasets. A prediction model is obtained using the training set, while the test set is held for empirical error estimation.</p>
<p>Unlike the 'statistical analysis' and 'Regression' courses in the Data Science certificate program, where we calculated the confidence interval using the same dataset we used to calculate the model, the confusion-matrix that we will calculate below, will use the 'test' data that we partition from the pml-training.csv file (and do NOT use in calculating the model) to evaluate the accuracy range for the prediction models.</p>
<!--begin.rcode
inTrain <- createDataPartition(y=train$classe, p=0.6, list=FALSE)
myTrain <- train[inTrain,]; myTest <- train[-inTrain,]
end.rcode-->
<h3>2.3 Clean Training Data Partition</h3>
<p>The device vendors did not create their device log files with the intent of supplying data just to R analysis programs. As a result, we will have to 'clean' the data before we can analyze it. 'Cleaning' performs multiple functions:<ul>
<li>Step-1 - Remove variables that cannot aid in categorizing activity performance because they do not vary from one observation to the next
<li>Step-2 - Remove variables whose corelation can only be accidental, and would have no meaning
<li>Step-3 - Remove variables that a majority of users do not collect, and that would therefore not provide a useful basis for categorizing future users going forward</ul>
<p>Since this report will be publicly available on GitHub, we would also remove any personal identifying attributes (PIA), but the HAR group has already done that for us.</p>
<p><u>Step-1: Remove NearZeroVariance variables</u><br>'Near Zero Variance variables' add nothing to the analysis because they did not change over the data collection period.</p>
<!--begin.rcode
NZVvars <- names(myTrain) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_pitch_belt", 
                                 "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", 
                                 "skewness_yaw_belt", "max_yaw_belt", "min_yaw_belt", 
                                 "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm", 
                                 "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", 
                                 "var_pitch_arm", "avg_yaw_arm", "stddev_yaw_arm",
                                 "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_pitch_arm", 
                                 "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", 
                                 "skewness_yaw_arm", "max_roll_arm", "min_roll_arm", 
                                 "min_pitch_arm", "amplitude_role_arm", "amplitude_pitch_arm",
                                 "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell",
                                 "skewness_roll_dumbbell", "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", 
                                 "max_yaw_dumbbell", "min_yaw_dumbbell", "amplitude_yaw_dumbbell", 
                                 "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                 "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", 
                                 "max_roll_forearm", "max_yaw_forearm", "min_roll_forearm", 
                                 "min_yaw_forearm", "amplitude_roll_forearm", "amplitude_yaw_forearm", 
                                 "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", 
                                 "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", 
                                 "avg_yaw_forearm", "stddev_yaw_forearm", "var_yaw_forearm")
myTrain <- myTrain[!NZVvars]
end.rcode-->
<p><u>Step-2: Remove the ID column</u><br>The 'ID' column is a record identifier. We would not know how to interpret any relationship that the analysis may find to a generated sequence number, so we are removing the ID column from the dataset. Specifically, any relationship to ID would indicate a relationship to the order in which observations were recorded, and if such a relationship were found, that would mean we are missing at least one attribute. That would not help this analysis since we seek to assess performance based on the attributes that are being collected.</p>
<!--begin.rcode
myTrain <- myTrain[c(-1)]
end.rcode-->
<p><u>Step-3: Remove columns that are 60% or more 'NA'</u><br>The course notes used 60% as an example of threshold, so I have used the same threshold. Rationale: If a majority of device users do not collect the attribute, we cannot rely on the attribute to categrize the performance of new users going forward.</p>
<!--begin.rcode
cntTrain <- length(myTrain)
for(i in cntTrain:1) { #start with the last column and go forwards
    if(sum(is.na(myTrain[,i])) / nrow(myTrain) >= 0.6) { #if more than 60% NA
        myTrain <- myTrain[ , -i] #remove the column
    }
}
dim(myTrain)
end.rcode-->
<h3>2.4 Clean the Testing Partition</h3>
<p>We now perform the same cleaning steps on the unit test data that I partitioned from the pml-training.csv file. I don't break out the steps since you have already seen them.</p>
<!--begin.rcode
myTest <- myTest[!NZVvars] #remove variables with near zero variance
myTest <- myTest[c(-1)] #remove the ID column

cntTest <- length(myTest)
for(i in cntTest:1) { #start with the last column and go forwards
    if(sum(is.na(myTest[,i])) / nrow(myTest) >= 0.6) { #if more than 60% NA
        myTest <- myTest[ , -i] #remove the column
    }
}
dim(myTest)
end.rcode-->

<h3>2.5 Clean the Acceptance Test Data Provided</h3>
<p>In this section we clean the acceptance test data provided in the file 'pml-testing.csv'. Because this is a new dataset, there are few additional considerations:</p>
<ul>
<li>Step-1 - We do have to apply the same cleaning steps as we applied to the training data
<li>Step-2 - The file 'pml-testing.csv' contains a column 'problem ID' that does not appear in the trainging data. We remove this column as the model built from the training data would not use it.
<li>Step-3 - The R 'read.csv' process defaults the class based on the initial data values it reads for each column. This can lead to inconsistencies between different extracts from the same source, when, for example, a character field just happens to contain all numerals. For this reason, we need to check that R is able to coerce subsequent extracts from the same source to the same classes for each column. If the coercion fails, we know that the data has really changed, and our analysis does not fail just because 'read.csv' inferred the column class differently.
</ul>
<p><u>Step-1: Clean the 'Test' Data</u><br>We must also apply the steps applied to clean the 'Training' data, to the 'Test' data for the acceptance test.</p>
<!--begin.rcode
test <- test[!NZVvars] #remove variables with near zero variance
test <- test[c(-1)] #remove the ID column

cntTest <- length(test)
for(i in cntTest:1) { #start with the last column and go forwards
    if(sum(is.na(test[,i])) / nrow(test) >= 0.6) { #if more than 60% NA
        test <- test[ , -i] #remove the column
    }
}
end.rcode-->
<p><u>Step-2: Remove 'problem ID' column</u><br>The 'Testing' dataset contains an additional column for 'problem ID' that is not part of the 'Training' dataset and that does not appear to contain information that is relevant to our analysis.</p>
<!--begin.rcode
test <- test[-length(test)] #remove the 'problem ID' column not found in the Training dataset
end.rcode-->
<p><u>Step-3: Force the 'Testing' dataset columns to have the same class as the 'Training' dataset</u><br>The RandomForest algorithm may fail on the 'Testing' dataset if the classes are not consistent with the 'Training' dataset. At a minimum, we prefer that the class coercion fail rather than the RandomForest procedure, because if the class coercion fails we know the data is really different.</p>
<p>Assumption: The acceptance test dataset (pml-testing.csv) has the same variables, in the same order. </p>
<!--begin.rcode
test <- rbind(myTrain[2, -58], test) #rbind will throw an error if the class coercion did not work
test <- test[-1,] #remove the row we just bound
end.rcode-->

<h2>Apply Machine Learning</h2>
<p>I will apply two methods of machine learning. The first, 'decision tree' is less accurate in execution but produces a useful diagram that will help readers of this report interpret the results. The second, 'random forest', is the method that will use to assess individual perormance as it is more accurate.</p>
<h3>Decision Tree</h3>
<p><u>Step-1: Build the Tree</u><br>I will use 'rpart', 'Recusive Partitioning and Regression Trees' package, to build the tree model and then call 'fancyRpartPlot', which is part of the 'Rattle' package, to display the tree.</p>

<!--begin.rcode fig.width=17, fig.height=8
modFitTree <- rpart(classe ~ ., data=myTrain, method="class")
fancyRpartPlot(modFitTree)
end.rcode-->
<p><u>Step-2: Evaluate the Tree</u><br>This step is analagous to a unit test: I will calculate a prediction based on the 'myTrain' data that I earlier partioned from the 'Training' dataset, and then calculate the 'confusion matrix'.</p>
<p>As you see in the output from the confusion matrix, the categorization of the observations in the unit test partition, based on the decision tree, is between 86.65% and 88.13% accurate. The matrix shows that even when wrong, the categorization is not very wrong for good performers: An 'A' may be categorized as a 'B' or 'C', but not as a 'D' or 'E'. However, the categorization varies more for poor performers ('D' and 'E').</p>
<p>Conclusion: The Decision Tree based on rpart is good-enough to build a diagram that explains the categorization logic, and would be sufficiently accurate for end-users who are good athletes ('A' and 'B' performance) and just want more feedback. However, the accuracy would <u>NOT</u> be sufficient for users who really need guidance ('D' and 'E' performance) as the risk of mis-categorization is higher for poor performance than for good performance.</p>
<!--begin.rcode
confusionMatrix(predict(modFitTree, myTest, type="class"), myTest$classe)
end.rcode-->
<h3>Random Forest</h3>
<p>The 'randomForest' R package implements Breiman's random forest algorithm. The code has stood the test of time: the current R package is based on Breiman and Cutler's original Fortran code for classification and regression.</p>
<!--begin.rcode 
modFitForest <- randomForest(classe ~ ., data=myTrain)
predictionsForest <- predict(modFitForest, myTest, type="class")
end.rcode-->
<p>As you see in the output from the confusion matrix, the categorization of the observations in the unit test partition, bansed on the randomForest, is between 99.7% and 99.9% accurate. More importantly, we see from the matrix that even when wrong, the categorization is never off by more than one category -- a 'D' may be categorized as a 'C' or 'E' in a very few cases, but never more.</p>
<p>Conclusion: People who really need guidance on their performance of exercise activities (those who score 'D' and 'E') could use the randomForest assessment to tell them for which activities they need to improve.</p>
<!--begin.rcode 
confusionMatrix(predictionsForest, myTest$classe)
end.rcode-->
<h3>Acceptance Test</h3>
<p>In this section, I apply the randomForest model created above to the Acceptance test data, 'pml-testing.csv', and generate the test results to be submitted, using the file creation code provided in the assignment.</p>
<!--begin.rcode
predictAcceptance <- predict(modFitForest, test, type="class")
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(predictAcceptance)
end.rcode-->

</body>
</html>
